# -*- coding: utf-8 -*-
"""Machine Learning Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QakpwBtrqyUZF8CiSUMdyfSlk3hgWVNL
"""

import gzip
import json
import pandas as pd

import nltk
import re

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

import tensorflow as tf

from sklearn.model_selection import train_test_split

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

nltk.download('stopwords')
stop_words = stopwords.words('english')
nltk.download('punkt')

num_words = 10000
max_length = 256

def find_sentiment(rate):

  if rate >= 3:
    return 1
  else:
    return 0

def clean_sentences(line):

  cleaned_list = []
  tokens = word_tokenize(line)

  for word in tokens:
    #if word not in stop_words:
    word = re.sub(r'(?<=\w)[.,!?;](?=\w)', ' ', word)
    word = re.sub(r'[.?!,;](?=\w)', '', word)
    word = re.sub(r'(?<=\w)[.,!?;]', '', word)
    word = re.sub(r'[^\w ]', '', word)

    if len(word) > 0:
      cleaned_list.append(word)

  cleaned_line = (" ").join(cleaned_list)
  
  return cleaned_line

def train_and_save_model(X_train, X_test, y_train, y_test):
  
  model = tf.keras.Sequential([
    tf.keras.layers.Embedding(num_words, 32, input_length = max_length),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(32, activation = 'relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(1, activation = 'sigmoid')
  ])

  model.compile(optimizer = 'adam', loss = tf.keras.losses.binary_crossentropy, metrics= ['accuracy']) 

  epochs = 20
  model.fit(X_train, y_train, batch_size = 256, verbose = 2, epochs = epochs, validation_data = (X_test, y_test))
  model.save('drive/My Drive/App Files/ML Files/EmotText Model.h5')

def tokenize_sentence(X):
  tokenizer = Tokenizer(num_words = num_words, oov_token='<oov>')
  tokenizer.fit_on_texts(X)

  X_seq = tokenizer.texts_to_sequences(X)
  X_padded = pad_sequences(X_seq, padding = 'post', maxlen = max_length)

  return X_padded

def train_data(filename):

  with gzip.open(filename, 'rt', encoding = 'utf-8') as input:
    df = pd.DataFrame([json.loads(line) for line in input])

  tokens = df['reviewText'].apply(clean_sentences)

  df['sentiment'] = df.overall.apply(find_sentiment)
  df['sentiment'] = df['sentiment'].fillna(0)

  X_train, X_test, y_train, y_test = train_test_split(tokens, df['sentiment'], test_size = 0.3, random_state = 39)

  tokenizer = Tokenizer(num_words = num_words, oov_token='<oov>')
  tokenizer.fit_on_texts(X_train)
  tokenizer.fit_on_texts(X_test)

  X_train_seq = tokenizer.texts_to_sequences(X_train)
  X_test_seq = tokenizer.texts_to_sequences(X_test)

  X_train_padded = pad_sequences(X_train_seq, padding = 'post', maxlen = max_length)
  X_test_padded = pad_sequences(X_test_seq, padding = 'post', maxlen = max_length)

  train_and_save_model(X_train_padded, X_test_padded, y_train, y_test)

filename = 'drive/My Drive/App Files/ML Files/video_game_reviews.gz'
train_data(filename)

"""The rest of the code is just me testing random stuff. Dw about it."""

ml = tf.keras.models.load_model('drive/My Drive/App Files/ML Files/EmotText Model.h5')
ml.predict_classes(tokenize_sentence(clean_sentences('This model is bad. I hate it')))

with gzip.open(filename, 'rt', encoding = 'utf-8') as input:
    df = pd.DataFrame([json.loads(line) for line in input])

